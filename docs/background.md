# Designing an AI-Powered Bushfire Simulation Inject Tool for Fire Service Training

**In Brief:** To enhance rural fire service training, a prototype simulation “inject” tool can combine geospatial mapping, real-world landscape data, and AI-generated visuals. Trainers will be able to draw a fire perimeter on a digital map, then generate photorealistic images or short video clips showing how a wildfire would look in that specific location. Key components include integrating detailed regional geospatial datasets (vegetation, topography, etc.), leveraging AI image/video generation models guided by structured fire behaviour parameters, and using 3D visualization for multiple vantage points (ground-level and aerial views). Below, we break down the research and technologies relevant to each aspect of this solution, with an emphasis on widely applicable sources and analogous tools used in emergency and defense training.

## 1. Drawing or Defining Fire Perimeters on Maps

Digital mapping tools used by fire agencies already support drawing fire perimeters (fire edges, burned area outlines, etc.). For instance, some agencies maintain custom GIS extensions for ArcGIS that provide a palette of standard bushfire mapping symbols and drawing tools. Using these incident mapping tools, GIS operators (or trainers) can manually sketch fire perimeters (polygons/lines) and other fire features on top of base map layers, such as topography and aerial imagery [11]. All fire features drawn in these tools are stored in a geodatabase and can be shared via the agency’s incident coordination systems for others to view in real time [11]. This approach - drawing a fire edge on a map to represent an ongoing incident - is common practice in both real operations and training exercises.

Modern web mapping libraries (e.g. Leaflet, Mapbox GL JS, Google Maps API) similarly allow users to draw polygons or polylines on an interactive map. The simulation inject tool can integrate such a library so that an instructor simply traces the desired fire perimeter on a digital map of the training area. The drawn polygon’s coordinates can then be used as a mask or input for further processing.

It’s worth noting that some fire simulation systems also allow users to define ignition locations or fire perimeters. For example, the Spark wildfire simulator (developed by CSIRO/Data61 for AFAC’s Australian Fire Danger Rating System) includes a GUI where analysts can import custom fuel maps and interactively input ignition locations or fire perimeter shapes, then run “what-if” spread simulations [10]. Legacy tools like Phoenix RapidFire (widely used in Victoria and by multiple agencies) also allow defining one or more fire ignition points to simulate spread across the landscape [10]. While these simulators focus on calculating fire growth rather than producing visual imagery, they underline the importance of an intuitive interface for defining the fire’s initial shape and location.

**Takeaway:** The new training tool can build on these examples by providing an easy-to-use map interface for drawing fire perimeters, using standard bushfire symbols and real map backgrounds for authenticity. The drawn shape will serve as a crucial input for generating visual simulations of fire in the specified area. This could simply be an outline on a 2D map, or ideally a polygon defining the burn area on a 3D terrain model of the location (to support later 3D visualization). Ensuring the tool uses the same symbology and terms that agency mappers and incident controllers use (as defined by the national AFAC mapping standards) will make the interface intuitive to trainers and volunteers [11].

## 2. Geospatial Data Sources for Local Landscapes (Vegetation, Topography, etc.)

To generate convincing wildfire visuals “in the context of the real landscape,” the system will need to draw on high-quality geospatial data for the selected location. Fortunately, state/territory and national agencies provide extensive geospatial datasets:

- **Topography and elevation:** State and territory governments often offer high-resolution digital elevation models (DEMs) and contour data via spatial services. These can be used to model terrain (hills, valleys, slope steepness) which significantly affect fire behavior and how a fire looks (fires burn faster uphill and terrain influences smoke columns). For broader coverage, national 5 metre and 30 metre DEM datasets are available. High-quality elevation data will enable draping imagery on a 3D terrain or calculating slope for fire behavior inputs.
- **Aerial and satellite imagery:** For visual realism, one can use base map imagery such as georeferenced aerial photos or high-resolution satellite images of the area. Many spatial services provide georectified aerial imagery layers at state/territory scale [11]. Other sources include open satellite imagery (e.g., Sentinel-2 or Landsat for recent fire scars or seasonal changes) and commercial providers that offer up-to-date high-res aerial imagery in populated areas. These images can serve as backgrounds on which the AI will paint the fire, or as textures in a 3D visualization.
- **Vegetation and fuel data:** State and national agencies often provide detailed vegetation and fuel maps. State vegetation type maps and related datasets classify vegetation communities and fuel types across a region [3]. Many agencies also maintain bush fire prone land datasets that delineate vegetation categories (Vulnerable, Highly Prone, etc.) for planning purposes [3]. For fire behavior modeling, national fuel maps categorize fuel types (grassland, forest, shrubland, plantations, etc.) consistently across jurisdictions. These maps can be used to infer fuel load and vegetation characteristics in the drawn fire area - information that can feed both fire behavior calculations and visual attributes (e.g., tall forest fires vs. grass fires look and behave differently).
- **Land use and infrastructure:** Contextual data like land use/land cover maps (e.g., distinguishing forests, farms, towns), road networks, building footprints, and even features like powerlines and water bodies are available through state/territory open data portals [11]. While not all of these directly influence fire visuals, they are useful for training context - for example, knowing where structures or roads are can help the AI generate views from a roadway or show a fire threatening buildings. Such data can also help select key vantage points (e.g., the location of a nearby highway, or a fire tower) from which to render ground-level perspectives.
- **Weather and climate data:** For completeness, the tool should access weather data for the area and scenario period. This might include historical weather records from the Bureau of Meteorology or live data if simulating current conditions. Key inputs are wind speed and direction, temperature, humidity, and drought indices (e.g., KBDI or Soil Dryness Index which affect fuel moisture) [13]. These parameters will inform the fire behavior and can be included in AI prompt text (e.g., “a wildfire driven by strong westerly winds under very dry conditions”).
- **Data access:** State/territory open data portals provide public access to many of the above datasets. For example, bush fire prone land maps are often downloadable via open data platforms. National hazards science hubs also compile bushfire-related datasets (fire extent and severity maps, vegetation maps, soil and erosion data, etc.) for recent fire seasons and statewide bushfire risk analysis [3]. Additionally, national-scale datasets (like fuel maps or climate data) can be obtained through national data repositories.

By integrating these layers, the simulation tool ensures that any fire perimeter drawn by a trainer is immediately contextualized with authentic landscape features. For instance, if an instructor draws a fire edge in a eucalypt forest on a steep slope, the system can recognize the vegetation type (e.g. dry sclerophyll forest) and slope steepness from the data, and later use that information to influence the fire’s visual characteristics and spread rate.

## 3. AI Tools/Platforms for Photorealistic Fire Imagery or Video (with Geospatial Context)

Recent advances in generative AI offer exciting possibilities for creating realistic fire imagery from minimal inputs. The simulation tool can leverage these models to turn the drawn perimeter and associated data into compelling visuals:

- **Image generation models:** Text-to-image diffusion models like Stable Diffusion (open-source) or DALL·E 3 and MidJourney (proprietary) can produce highly realistic images of landscapes and wildfires. By providing a carefully crafted textual prompt (and, optionally, reference images or sketches), these models can render scenes of burning forests, grasslands, smoke-filled skies, etc. For more control, Stable Diffusion’s ControlNet extension allows using additional inputs (conditioning) such as a segmentation mask or a rough outline to guide where fire should appear in the image. In this case, the drawn fire perimeter can be converted into a binary mask (fire area vs. background) and input to a ControlNet-enabled model that is trained to treat the mask as “fire.” Research has shown that diffusion models guided by masks can successfully integrate precise wildfire locations into generated images while preserving the surrounding scene’s features [16]. This means the AI could realistically “paint” flames and smoke exactly along the trainer’s drawn fire edge on the actual landscape background.
- **Specialized wildfire image synthesis:** Beyond general-purpose models, there are domain-specific AI approaches. SynFAGnet (2024), for example, is a generative adversarial network designed to create realistic fire images by automatically blending fire into background photos. It uses an object-placement network to decide where and how to add flames, then a GAN to render fire that matches the scene (adding glow, reflections, etc.), and notably accepts an input mask to define the fire’s shape [17]. This aligns perfectly with using a drawn perimeter as the mask. Similarly, a 2024 study from Clemson University introduced “FLAME Diffuser,” a diffusion-based method that generates wildfire images guided by masks derived from real fire data and Perlin noise. Their approach demonstrated that injecting a fire-front mask into the diffusion process yields realistic flame placement and high visual fidelity [16]. These cutting-edge techniques are primarily research projects, but they indicate that it’s feasible to develop a custom AI model for the tool - or fine-tune an existing model - to accept geospatial inputs (like maps, masks, or elevation data) and output a photorealistic wildfire image.
- **Photorealistic video generation:** Generating short video clips of wildfires in specific landscapes is more challenging, but it’s an active area of development. Text-to-video AI models (such as Runway Gen-2 or Meta’s research on Make-A-Video) are emerging, though they are still quite limited in resolution and control. An alternative approach is to generate a series of still images (or frames) from the AI model - for example, varying the viewpoint or simulating slight temporal changes - and then stitch them into a video. There is also promising research into specialized methods for wildfire video simulation. For instance, the company Reelmind.ai (cited by 2025) offers an AI-powered wildfire video generator that claims to create “hyper-realistic, customizable fire scenarios” by combining AI image generation with computational fluid dynamics models [1]. Users can specify parameters like wind speed, fuel type, and terrain, and the system produces a video of the fire spreading accordingly for training or entertainment purposes. While these capabilities are nascent, they suggest that a prototype could incorporate a simple video mode. For example, the tool could generate an animated progression by producing a sequence of images (e.g., showing the fire at 5-minute intervals moving across the drawn perimeter) or creating a panoramic video from different angles. As AI video technology matures, one could integrate a service or model to directly generate smooth wildfire animations from the scenario data.
- **Game engine rendering:** Another approach to achieve photorealism is using gaming or simulation engines (like Unreal Engine or Unity) with visual effects. High-end training systems sometimes use physics-based fire simulation combined with 3D rendering - for instance, a university and fire agency iFire project translates outputs from the WRF-SFIRE and Spark fire models into an immersive 3D environment using the Unreal Engine, producing high-fidelity visuals of fires in real landscapes [6]. Similarly, the XVR On Scene platform’s new “Fire Front” module uses a 3D engine to simulate wildfire spread and visuals in real time, allowing instructors to “lock or change factors like wind or humidity and see the flame front alter within seconds” [2]. For our prototype, a full physics-based simulation may be beyond scope, but we could consider a lighter approach: for example, using a game engine to place animated fire and smoke effects along the drawn perimeter on a 3D terrain model of the site. This could produce short movie-like visuals from any angle. Game engines also support particle systems for fire/smoke and can render scenes to video frames. The drawback is the development complexity - however, there are existing wildfire behavior plugins and even open-source projects (such as NVIDIA’s FireBlade or other academic GPU fire simulators) that might be repurposed.
- **Summary:** A hybrid strategy could be employed: use GIS data to set the stage (terrain, vegetation, etc.), simple fire behavior equations or a spread model for basic dynamics, and AI image synthesis for the final rendering. For example, one pipeline might: 1) use the drawn perimeter as an initial fire area, 2) run a fast cellular automata fire spread model for a short period to expand the fire or generate a smoke plume footprint, 3) feed that information into a text prompt (e.g., “bushfire covering 50 hectares of dry eucalypt forest, flames 10m high, thick smoke blowing eastward, late afternoon sun”) and a mask to guide an image generator, and 4) output a set of images from various perspectives. This kind of approach would have been very difficult a few years ago, but with today’s AI tools it is increasingly viable.

## 4. Incorporating Structured Fire Behaviour Parameters into Prompts

To ensure the AI-generated visuals and any simulation components are true-to-life, it’s critical to incorporate the same variables that firefighters use to predict and describe fire behavior. In Australian bushfire management, fire behavior is typically understood as a function of fuel, weather, and topography1. Key parameters to consider for each scenario include:

- **Fuel type and load:** What is burning? Grass fires behave and appear very differently from forest fires. Fuel data from vegetation maps (e.g., “grassland,” “dry sclerophyll forest,” “plantation forest,” etc.) can be translated into descriptors for the AI prompt (e.g., “a fast-moving grassfire” vs. “an intense crown fire in tall eucalypt forest”). Fuel load (quantity of burnable material, often measured in tonnes per hectare) influences flame height and smoke volume - higher loads (e.g., in heavy forest) produce taller flames and thicker smoke. Published fuel load estimations for various vegetation types [12] and the Keith vegetation classification system can inform these details. For instance, a prompt might include language like “a wildfire burning through dense, dry eucalyptus forest with heavy fuel on the ground” to reflect high fuel loads.
- **Weather conditions:** Wind is one of the most critical factors - it dictates the fire’s spread direction and smoke movement. The prompt should reflect wind speed and direction, e.g., “strong westerly winds driving the flames eastward” (if 30 km/h winds from the west) or “calm winds causing a towering vertical smoke plume.” Temperature and humidity affect fire intensity and should be reflected if extreme (e.g., “on a 40°C day with very low humidity, the fire burns ferociously”). If available, the tool can take real weather readings or forecasts from Bureau of Meteorology APIs for training scenarios. In structured form, agency fire analysis requests capture these as on-site weather observations (temperature, humidity, wind speed/gusts, wind direction, etc.) [13]. Time of day can also be relevant - a night fire might be described with large flames visible against darkness, whereas a daytime fire might emphasize smoke plumes and an orange sky.
- **Topography:** Steep terrain significantly amplifies fire behavior - fires burn uphill faster and produce longer flame lengths. The tool should determine the slope of the ground within the fire perimeter (from the DEM). For instance, if the fire is on a hill or in a valley, the prompt could note “fire on a steep slope” or the image generator could be instructed to show flames “racing up a hillside.” If using a 3D engine or GIS, the elevation data will directly shape the fire’s rendered position and the perspective for visualization. Topography also allows identifying strategic viewpoints (e.g., a camera on an opposite slope looking at the fire across a valley).
- **Fire behavior indices:** To add more nuance, the tool could optionally incorporate outputs from fire behaviour models or empirical formulas. For example, the rate of spread (ROS) could be estimated using models like the McArthur Grassland/Forest Fire Danger Meters or the CSIRO fire spread equations, based on input fuel and weather. While full simulation is beyond the initial scope, simple calculations could yield, say, an estimated ROS of 500 m/h and flame height of 10 m - which the prompt might translate into qualitative terms (“rapidly spreading” or “moderate rate of spread”, “3-meter flames” etc.). The Australian Fire Danger Rating System (AFDRS) provides Fire Behaviour Indices (like Fire Behaviour Index (FBI) and Fire Danger Ratings) that categorize conditions from “Moderate” to “Catastrophic” based on fuel, weather, and ignition difficulty (GeneralVariable-DataAndMethods.pdf). These could be used behind the scenes to adjust the scenario description (e.g., a catastrophic fire danger day might automatically trigger descriptions like “extreme fire conditions with erratic behavior and long-range spotting”).
- **Fire stage and suppression actions:** Trainers might also want to simulate specific conditions like an early-stage fire vs. a large, established blaze. The prompt could reflect this (a small grassfire vs. a massive wildfire front). If the scenario includes firefighting efforts, the tool might need to depict water or retardant drops, backburn lines, etc., though that adds complexity. In future, coupling with a spread simulator could allow the inject tool to show the fire’s growth over time or in response to suppression (some advanced training platforms like XVR do model how a virtual fire reacts to a wind change or a back-burn in real-time [2], but that requires a robust simulation engine).
  In practice, these parameters can be fed into the AI in two ways: (1) Directly in the text prompt – e.g., “a wildfire on a 33° slope burning through dry pasture (75% cured) with 40 km/h northwesterly winds, under extreme fire weather conditions” – or (2) implicitly via a model trained on fire science data – for example, a custom AI image generator might take numeric inputs (wind speed, fuel moisture, etc.) and internally adjust the image (this would require training data or rules linking those inputs to visual outcomes). Initially, a simpler text-based approach is viable: the tool can automatically compose a prompt sentence from the structured inputs the trainer provides. Reference to official frameworks can guide this: for instance, agency fire behaviour analysis request forms show the key factors to capture (location, elevation, aspect, fuel type, fuel load, drought factor, grass curing, temperature, humidity, wind, etc.)13. Ensuring the prompt covers these will result in more comprehensive and realistic outputs. Over time, user feedback can refine which details most impact the visuals so the prompt templates can be tuned accordingly.

## 5. 3D Mapping and Visualization Tools for Integration

Because the training goal is to help participants visualize the fire in the real landscape, a 3D mapping or simulation component is highly beneficial. There are a few paths to achieve this:

- **Web-based 3D mapping (WebGIS):** Libraries like CesiumJS provide a browser-based virtual globe that can stream high-resolution terrain and imagery. Cesium can take the elevation data and aerial imagery of a location to render a 3D scene. The drawn fire perimeter (as a polygon) could then be extruded or textured in this 3D space (for example, showing a translucent red “fire zone” on the terrain). While Cesium by itself won’t create a photorealistic flame, it could be a platform to overlay simpler visual effects (semi-transparent fire polygons or particle effects) and allow interactive camera movement. Cesium has been used in the past for visualizing geographic phenomena in 3D, and even wildfires - e.g., Lockheed Martin’s Joint Activity Manager uses Cesium to track wildland fires with AI, integrating data like satellite heat detections and perimeters on 3D maps. A custom Cesium app could similarly show the fire’s location and extent on a 3D map of the training area, and perhaps animate its spread based on simple models or expert input.
- **Game engines (Unreal/Unity) or simulation software:** A more advanced option is to use a game engine or dedicated simulation software to handle visualization. This is how some high-end training systems work. For example, a fire and rescue iFire project creates a fully immersive 3D environment with realistic flames and smoke by using the Unreal Engine and custom AI visualisation tech (developed by a university lab), all grounded in real geospatial data [6]. In a similar vein, commercial firefighter training platforms like XVR On Scene (used by many fire agencies worldwide) allow instructors and trainees to navigate a 3D virtual fireground. XVR’s recent version introduced “Fire Front,” a wildfire simulation module that uses terrain profiles, plant types, fuel load, and wind data to model flame spread and behavior in a 3D environment [2]. Instructors can introduce changes (like a wind shift or a new fire ignition) and the virtual fire reacts in real-time, forcing trainees to adapt strategies [2]. XVR also includes a Photo Mode for capturing high-quality images from any viewpoint in the 3D scene (useful for creating injects or post-exercise debrief materials) [7].
- **Augmented or mixed reality:** In the future, one could envision AR integration - for instance, overlaying a simulated fire onto a real map or even the physical environment. While not immediately required, some experimental training systems have used AR; for example, Simtable projects fire spread simulations onto a sand table, creating a tangible 3D map that firefighters can physically interact with while a computer projects flames and smoke on it. Simtable uses agent-based modeling and local GIS data to simulate fire growth under varying wind, terrain, and fuel conditions, enabling “what-if” scenario experimentation in a 3D landscape model [4].
  For an initial prototype, a practical approach may be to start with generating static 3D views (e.g., fixed-perspective renders or 360° panoramas) rather than a fully interactive VR experience. However, designing the system with a 3D engine or mapping platform from the outset could allow easier expansion to interactive or immersive modes later. Even in the near term, having a 3D representation means the tool can automatically produce multiple angle shots – e.g. one image from a firefighter’s eye level on a road, another from an aerial perspective – without needing separate user sketches for each view. This directly addresses the desire for multi-angle visualization (ground, hilltop, aircraft views). If using a game engine, one could predefine a few camera positions (such as “nearby road at 1.8m height”, “drone at 100m AGL looking north”, etc.) and have the system render the fire from each.

## 6. Reference Documents and Frameworks for Structured Prompt Design

To create structured and effective prompts (and scenario definitions) that align with fire service training, it’s important to incorporate trusted reference frameworks and terminology:

- **Australian wildfire doctrine and terminology:** Australia has a well-established common lexicon for wildfire (bushfire) terms, primarily through AFAC (Australasian Fire Authorities Council). The AFAC Bushfire Glossary [14] is a comprehensive list of standard definitions for fire behavior terms and phenomena. Using this terminology in AI prompts and scenario descriptions will ensure consistency with what trainees have learned. For example, terms like rate of spread, flame height, flank, head fire, spotting, crown fire, etc., all have specific meanings that could be included in the descriptions. Adhering to these standard definitions prevents confusion - for instance, distinguishing a “grassfire with moderate rate of spread and 2 m flame heights” versus an “extreme crown fire with profuse spotting.” Incorporating such phrasing in prompts helps the AI focus on producing the correct visual severity and style of fire, and resonates with trainees’ understanding.
- **Incident Command System (AIIMS) formats:** The tool’s design can borrow from how scenarios are documented in the Australasian Inter-Service Incident Management System (AIIMS). Typically, an Incident Action Plan or a Situation Report will include a clear description of the situation (location, conditions, fire status) and any evolving factors. For a training inject, a structured format might include sections like “Fire location and size,” “Fuel/Vegetation,” “Weather conditions,” “Topography,” “Predicted fire behavior,” and “Resources in place.” While the primary aim is visual output, providing a text summary to accompany the images/video - essentially auto-generating a mini scenario description - can be very useful for instructors and participants. This could mirror the fields in agency fire behaviour analysis request forms (see Section 4 above) to ensure all key factors are covered [13]. Additionally, documents like operational briefing and debriefing guidelines or training scenario templates might offer insight into how scenario information is typically structured (e.g., an acronym like SMEACS - Situation, Mission, Execution, Administration, Command, Support - used in briefings).
- **Bushfire training manuals and research:** Training information booklets and fire service training manuals may contain scenario examples or guidance on exercise design, though these are often focused on learning outcomes and safety rather than technology. Nonetheless, they underscore what information matters most in a scenario (e.g., “trigger points” for evacuations, which could be integrated by showing a fire approaching a certain landmark). Also, bush fire protection planning guidelines classify vegetation and fire behavior thresholds used in risk assessments. These classifications (for instance, forest types and their fuel loads as in PBP 2019’s appendices [12]) could inform the way the tool describes fuels in prompts. Likewise, AFAC’s Bushfire Simulation literature (AFAC has published case studies on using simulations in training and operations [10]) might provide quality benchmarks - e.g., emphasizing the need for accuracy in weather and fuel inputs for any simulation.
  In essence, using official references ensures the AI’s output is not only realistic but also pedagogically sound. It aligns the simulated scenario with both the science of fire behavior and the training language familiar to fire service personnel. By structuring the AI prompts with this standardized information (perhaps in a template form), the output will more likely meet training objectives and can be directly supported by existing training materials.

## 7. Similar Simulation/Visualization Tools in Emergency Services and Defense

It’s valuable to learn from existing simulation and visualization platforms that pursue similar goals. Here are a few relevant examples and their key features:

| Tool/System                                        | Description and Use Case                                                                                              | Notable Capabilities/Insights                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| -------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| MapDesk (Agency example)                           | Desktop GIS extension for incident mapping [11], used in real operations and training.                                | Allows drawing fire perimeters using standard bushfire map symbols (points, lines, polygons) [11]. Integrates a library of base layers (topography, roads, place names, aerial imagery, etc.) covering the region [11], for realistic map context. Stores incident data in geodatabases for sharing via agency incident coordination systems [11].                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| Spark and Phoenix RapidFire (Australia)            | Wildfire spread simulation models used by fire agencies (mostly for analysis/planning).                               | These compute fire growth over time using input layers (fuel, weather, terrain). Spark (CSIRO/Data61 and AFAC) features a modular API/GUI to run “what-if” scenarios; users can import custom fuel maps, modify weather, and interactively set ignitions or fire boundaries on a map [10]. Phoenix RapidFire (Univ. of Melbourne/Vic. Dept.) is a well-established simulator supported by multiple agencies [10]. While they output 2D maps (not photo imagery), their predictions (rate of spread, intensity, ember distribution) could inform an AI prompt or be visualized via simple icons on the map.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| SimsUshare and Fire Studio (International)         | 2D photo-based fire simulators for training (used by fire services and military for scenario drills).                 | These tools let instructors import real photographs of local sites and then overlay animated fire and smoke elements. They provide libraries of fire, smoke, and explosion effects that can be placed on the image via drag-and-drop [98]. Instructors can script the fire’s progression over time (e.g., smoke growing, fire spreading to new areas) to create a “playable” scenario. While not tied to maps or geodata, they demonstrate the value of using local imagery and simple overlays to achieve realism in training. Our proposed tool can be seen as the next evolution: automatically generating those fire effects on images using AI instead of manual placement.                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Simtable (USA)                                     | Interactive sand table for wildfire and emergency simulations, used in firefighting and disaster management training. | Employs a projector and a physical sand table to create 3D terrain models. Simtable’s software then projects simulated fire spread onto the sand, driven by real GIS data and adjustable parameters. It models fire growth based on wind, terrain, and fuels, enabling users to watch how a fire might propagate across their “own high-risk area” in real time [4]. Trainees can test strategies like fuel breaks or evacuations in a tangible 3D space. This system underscores the importance of using local geographic data for realism and allows instructors to “inject” changes (wind shifts, new ignitions) dynamically. Our design similarly prioritizes local data and flexibility, albeit with digital rather than physical 3D maps.                                                                                                                                                                                                                                                                                                                                                                                                         |
| XVR On Scene - “Fire Front” (Global, incl. AU/NZ)  | Immersive 3D/VR emergency response training platform with a wildfire module (used by many fire and rescue services).  | Provides a 3D virtual environment where entire incident management teams can train together. The new wildfire simulation (Fire Front) is built on scientific models and lets instructors manipulate variables like wind, fuel moisture, and topography to see immediate effects on fire behavior and spread [2]. For example, a sudden wind change can be applied mid-exercise, and the virtual fire’s spread will accelerate or change direction, forcing participants to adapt in real time [2]. XVR also supports multi-agency training (police, ambulance, etc.) in the same scenario, reinforcing joint operations [2]. A built-in Photo Mode allows capturing snapshots from any angle for post-exercise debriefs or additional inject materials [7]. While XVR is a comprehensive (and commercial) solution beyond the scope of a single-inject tool, its success shows the value of realistic, adjustable visuals for training. Our prototype can take inspiration from XVR’s approach by starting with instructor-defined scenarios (manually drawn fire and chosen conditions) and focusing on high-quality visualization of those scenarios. |
| State Fire & Rescue/University “iFire” (Australia) | Research prototype of an AI-driven 3D bushfire training simulator (state fire and rescue service, 2024) [6].          | Combines advanced fire science models with immersive visualization. It uses Data61’s Spark and the WRF-SFIRE atmospheric model to simulate fire behavior, and renders the results in a 360° panoramic 3D cinema using the Unreal Engine [6]. Trainees can experience a virtual bushfire in a specific real-world location and adjust variables (wind, fuel load, temperature) to see how the fire reacts in real time [6]. This R&D project highlights the cutting edge of what’s possible: real physics-based fire simulation married with cinematic visuals. While such complexity may be a long-term aspiration, elements like the use of actual forecast models (for wind, etc.) or high-end 3D graphics could trickle down into the inject tool over time.                                                                                                                                                                                                                                                                                                                                                                                         |
| Aviation Firefight Simulator (AFCTS) (Agency)      | Air Attack Supervisor Training Simulator using VR (by Virtual Simulation Systems).                                    | Focused on aerial firefighting coordination, this system places trainees in a VR aircraft cockpit. It features a realistic 3D environment, multiple aircraft (water bombers, helicopters) and can simulate “a wide variety of aircraft operations under different environmental conditions” [5]. Instructors can place static or progressive fires in the virtual landscape and let students practice tactics like aerial suppression drops under controlled conditions [5]. This specialized tool demonstrates how specific training needs (air attack) can be met with simulation. For our purposes, it suggests the value of having the option to view the fire from an aerial perspective, especially for incident management training (since fire services often use aircraft for reconnaissance and water/retardant drops).                                                                                                                                                                                                                                                                                                                       |

_Table: Examples of relevant simulation and visualization tools in fire/emergency training. Australian examples are emphasized where available._

Each of the above systems tackles the challenge from a slightly different angle – from map-based fire spread modeling to fully immersive VR. Our proposed training inject tool lies somewhere in between: it would not replace full simulators like Spark or XVR, but rather complement training by providing quick, visual “slices” of a scenario that instructors can drop into exercises (e.g., during a tabletop incident management exercise, periodically show an updated “photo” of the fire’s progress based on what the trainees decide). The research and technologies behind existing tools can guide development: for instance, using local GIS data for realism4, exposing key variables for instructor control2, and leveraging AI to generate visuals where manual creation would be too slow or impractical.

## Conclusion and Next Steps

Designing a simulation inject tool for bushfire training is an ambitious but achievable project with today’s technology. By uniting GIS mapping capabilities, rich geospatial data, and AI-driven image generation, such a tool can produce strikingly realistic wildfire visuals grounded in real-world local landscapes. In practice, an instructor could sketch a fire perimeter on a map of a training area, input conditions (e.g. “midday, 35 °C, strong NW wind, extreme fire danger in dry eucalyptus forest”), and within minutes receive AI-generated photographs or video clips depicting the evolving fire. These visuals would be used during training to test and improve firefighters’ decision-making – bridging the gap between abstract map-based exercises and the visceral reality of a fireground.

To move forward, agencies (or developers) should consider a phased approach:

1. **Prototype phase:** Start with a focus on static image generation. Develop the web interface for map drawing and integrate it with a stable diffusion model (possibly fine-tuned on wildfire imagery). Use abundant regional data to automatically craft prompts and masks. Evaluate the realism of outputs with experienced fire service personnel and iterate on the prompt engineering. Ensure the images are credible and align with expected fire behavior for the inputs.
2. **Data integration:** Incorporate regional open data early. For example, automatically pull the vegetation type at the drawn location (using a WMS or vector query to a state vegetation map [3]), get the day’s weather from local meteorological sources, and compute simple fire behavior metrics. This not only improves the output but makes the tool a quick teaching aid for understanding how different factors influence fire appearance (e.g., try drawing the same fire on a mild day vs. a severe day and compare the AI images).
3. **3D visualization:** As a stretch goal, integrate a 3D engine or mapping tool to support multiple viewpoints. Even without full VR, being able to generate a pseudo-3D view (e.g., oblique view from a hill) will add value. This could be done by coupling the AI image generator with depth maps from a DEM, or by using the engine to render non-burning scenery and then applying AI fire effects within the image.
4. **Progressive scenarios:** In later development, consider linking with a fire spread model (even a simplified one) to auto-grow the drawn fire over time. This way, a series of injects can be created (e.g., showing the fire after 30 minutes, 1 hour, 2 hours, etc., following likely spread patterns). The tool could then generate a timeline of images or an animation, providing a quasi-dynamic simulation. This would align with how some training exercises unfold, where the situation updates as time progresses.
5. **Validation and safety:** Throughout development, involve training officers and fire behavior analysts to validate that the AI’s outputs are plausible and educational. The aim is not to predict fire outcomes with high fidelity (that’s what full simulators like Spark are for), but to provide visually and scientifically credible depictions that prompt realistic decision-making. Any AI-generated content would need review to ensure it doesn’t inadvertently show impossible fire behavior (or mislead trainees). Over time, a library of “approved” AI-generated images/videos could be built, corresponding to various standard scenarios (e.g., “high-intensity grassfire”, “slow-moving controlled burn in woodland,” “extreme crown fire under northerly wind,” etc.).
   By grounding the tool in authoritative data and fire science - while harnessing the power of modern AI for visualization - fire services can significantly enhance training realism. Instructors will gain a versatile scenario-building app to vividly illustrate fire conditions, and trainees will benefit from a more immersive experience that builds their intuition and preparedness for real bushfires. The convergence of GIS, fire modeling, and AI imagery is a cutting-edge approach, but one that holds great promise for improving emergency response training more broadly. This research-backed design provides a roadmap for turning that vision into a practical, powerful training tool [21].
